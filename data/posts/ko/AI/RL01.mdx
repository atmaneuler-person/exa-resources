---
title: 'RL1. The History of Reinforcement Learning'
date: '2025-12-20'
images: ['/static/images/RL01.png']
tags: ['강화학습', 'RL']
authors: ['default']
draft: false
summary: '강화학습(Reinforecemnt Learning: RL)을 본 블로그의 첫 주제로 선택한 이유는 최근 급격히 발전하고 주목받는 딥러닝과 LLM AI 에이전트들과는 다른 궤도에서, 전통적인 이론을 바탕으로 현실세계에 지속적인 영향을 끼치며 발전해온 강화학습에 대한 개인적인 관심과 Exavion의 확장된 ERP, AI 통합 기업 어플리케이션 Exa-Solution에 강화학습이 통합되는 것 또한 주된 이유 중의 하나이다'
---

강화학습(Reinforecemnt Learning: RL)을 본 블로그의 첫 주제로 선택한 이유는 최근 급격히 발전하고 주목받는 딥러닝과 LLM AI 에이전트들과는 다른 궤도에서, 전통적인 이론을 바탕으로 현실세계에 지속적인 영향을 끼치며 발전해온 강화학습에 대한 개인적인 관심과 Exavion의 확장된 ERP, AI 통합 기업 어플리케이션 Exa-Solution에 강화학습이 통합되는 것 또한 주된 이유 중의 하나이다.

강화학습의 역사부터 출발해, 이론과 주요 알고리즘을 해부하고 정리하여, 이를 실제 소프트웨어에 적용하는 전 과정을 탐구하는 것이 이 강화학습 시리즈의 목표이다.

# <b>강화학습의 역사</b>

컴퓨터는 1940년대에 등장했고, 1970년대에는 개인용 컴퓨터의 보급이 본격적으로 확산되기 시작했다. 1950~60년대, 초기 컴퓨터 과학자와 소프트웨어 엔지니어들은 컴퓨터가 계산만 잘하는 기계에서 더 나아가, 인간처럼 사고할 수 있는 인공지능을 만들 수 있지 않을까라는 질문을 던지기 시작했다. 이 시기에는 데이터 기반의 예측과 분류를 수행하는 머신러닝 학파와 함께, 시행착오(Trial-and-Error)를 통해 최적의 행동을 학습하고자 하는 강화학습 학파가 동시에 존재했다. 이미 1950년대부터 머신러닝과 더불어 강화학습에 대한 연구가 병행되었으며, 2000년대에 들어서는 우리가 현재 알고 있는 대부분의 핵심 이론과 알고리즘이 이미 완성된다. 이후, 강화학습은 딥러닝과 결합하며 다시 한 번 비약적인 발전을 이루게 된다.

## 전통적인 강화학습

오늘날의 강화학습 알고리즘은 대부분 2000년대 이전의 전통적인 이론과 구조를 기반으로 구축되어 있다. 이러한 전통적 강화학습(Classical RL) 알고리즘들은 주로 Tabular 방식으로 구현되었는데, 이는 상태와 행동의 값을 테이블 형태로 메모리에 저장하고 갱신하는 방식을 의미한다.

-

당시에는 컴퓨터 메모리 용량과 연산 성능의 한계로 인해, 이러한 알고리즘은 실험실이나 이론적 모델에 국한되어 있었으며 현실에 적용하기에는 어려움이 많았을 수 밖에 없었다. 하지만 시행착오를 통해 최적의 행동을 선택하려는 “최적 정책(Optimal Policy)” 개념이 제안되면서, 강화학습은 점차 정형화된 이론 체계를 갖추게 된다.

강화학습의 핵심 개념은 본질적으로 동물이나 인간이 환경과 상호작용하며 학습하는 방식과 매우 유사하다. 이러한 원리를 수학적으로 정의하고 체계화하기 위해, 리처드 벨만(Richard Ernest Bellman)은 가치 함수(Value Function) 개념과 함께 이를 계산하는 벨만 방정식([Bellman Equation](https://en.wikipedia.org/wiki/Bellman_equation))을 제안하였다. 그는 이 방정식을 풀기 위한 방법으로 동적 계획법([Dynamic Program](https://en.wikipedia.org/wiki/Dynamic_programming), DP)을 발표하였고, 이는 강화학습 이론의 출발점이 되었다.

벨만 방정식은 행동(action), 보상(reward), 정책(policy), 상태(state) 간의 연속적인 관계로 구성되며, 이로 인해 강화학습 문제는 마르코프 결정 과정(Markov Decision Process, MDP)으로 모델링되고, 동적 계획법으로 해석이 가능해졌다. 이후 20여 년이 지난 1970년대 후반에는, 강화학습의 또 다른 전환점인 시간차 학습(Temporal Difference Learning, TD-Learning) 개념이 등장하게 된다. TD 학습은 몬테카를로 방법(Monte Carlo Method)과 동적 계획법(DP)의 장점을 결합한 방식으로, 현대 강화학습의 핵심 요소로 자리 잡았고. TD 학습은 이후 등장할 수많은 알고리즘의 근간이 되었다. TD는 강화학습을 이해하기 위해 반드시 숙지해야 할 개념이다.

## 딥러닝과의 만남, 강화학습의 재도약

1989년, Chris Watkins는 논문 Learning From Delayed Rewards를 통해 Q-Learning 알고리즘을 제안하였다. 이 알고리즘은 Temporal Difference (TD) 학습의 여러 방식 중에서도 off-policy 방식을 따르는 대표적 방법이다. Off-policy 알고리즘은 행동을 선택하는 정책과 학습에 사용하는 정책이 서로 다를 수 있는 구조로, 하나의 정책으로 더 나은 행동(action)을 학습하면서, 실제 행동 선택에는 다른 정책을 적용하는 방식이다.

2000년 이후 딥러닝이 비약적인 성과를 거두기 시작하면서, 기존의 테이블 기반 강화학습에 신경망(Neural Network)이 결합되기 시작했다. 구글 딥마인드는 2015년, 딥러닝의 성과를 수용하며 Q-Learning을 딥 뉴럴 네트워크로 구현한 Deep Q-Network (DQN)으로 확장하였다. 당시 딥마인드는 Atari 게임 중 하나인 블록 격파 게임에 이 알고리즘을 적용하였고, 인간의 개입 없이 스스로 학습해나가는 인공지능의 모습을 통해 세계를 놀라게 했다. 이후, Q-Learning의 과도한 가치 추정(overestimation) 문제를 해결하기 위한 Double DQN이 등장하였다

https://youtu.be/EfGD2qveGdQ

이후 구글 딥마인드는 Reinforce 알고리즘에 딥러닝을 결합한 AlphaGo를 발표하며 강화학습의 잠재력을 전 세계에 또 한번 각인시킨 바 있다.

https://www.youtube.com/watch?v=WXuK6gekU1Y

https://www.youtube.com/watch?v=fKBes088KV4

https://www.youtube.com/watch?v=pJPdW8WWAso&t=1s

딥러닝은 기존 강화학습이 안고 있던 메모리 한계를 돌파하는 열쇠가 되었고, 이를 계기로 강화학습은 다양한 실제 분야에 본격적으로 적용되기 시작했다. 이러한 흐름은 테슬라 자율주행 차량, 로봇 제어 시스템, 게임 에이전트, 산업 자동화 등 실제 환경에서의 성공 사례로 이어졌고, 2017년 이후로는 딥러닝 기반 강화학습이 새로운 표준이 되었다.

-

2016년, 딥마인드는 또 하나의 중요한 전환점이 되는 Policy Gradient 방법을 기반으로 하는 A3C(Asynchronous Advantage Actor-Critic) 알고리즘을 발표하였다. 초기 강화학습 알고리즘들은 대부분 행동 가치 함수(Action-Value Function)를 근사하여 최적의 행동을 선택하는 Q-Learning 계열이었다면, A3C 이후의 현대 강화학습의 트렌드는 이 시점을 기점으로 행동의 가치 대신 행동 자체를 직접 근사하는 Policy Gradient 방식으로 발전이 가속화되기 시작했다. 현대의 대부분 강화학습 알고리즘들은 Policy Gradient 계열을 중심으로 설계되며, Q-Learning은 보조적 수단으로 활용되는 경향이 두드러졌다. 이는 2016년 이전, Q-Learning이 주류이던 시기와는 확연히 다른 흐름이다. 2020년 이후 수많은 논문들이 쏟아져 나왔고, 현재도 강화학습은 활발히 진화 중이다. RL의 전성시대가 본격적으로 열린 셈이다.

## 강화학습 이론의 정립과 확산

강화학습의 기초가 되는 벨만 방정식(Bellman Equation)은, 의사결정의 순차적 흐름을 수학적으로 모델링하기 위해 등장하였다. 벨만은 상태(state), 행동(action), 보상(reward), 그리고 다음 상태로 이어지는 일련의 과정이 MDP(Markov Decision Process) 형태를 따른다고 정의하였다. 이를 해결하기 위한 계산 방법으로 동적 계획법(Dynamic Programming, DP)을 고안하였고, 이는 이후 다양한 문제에서 폭넓게 활용되었다. DP 알고리즘은 본질적으로 문제를 더 작은 하위 문제로 나누어(recursive 하게) 해결하는 방식으로 작동한다. 이 점에서, 문제 해결에 리커시브한 특성이 포함되어 있다면 DP는 매우 강력한 접근법이 될 수 있다. 덕분에 DP는 강화학습을 넘어 다양한 컴퓨터과학 및 산업 응용 분야에서도 널리 사용되는 핵심 기법이 되었다.

한편, 딥러닝과의 결합 이전까지 강화학습은 오랫동안 이론적으로는 흥미롭지만 실용성이 제한된 분야로 인식되었다. 메모리 용량과 계산 자원의 제약 탓에 현실에 적용하기 어려웠기 때문이다. 이러한 상황 속에서 Richard S. Sutton과 Andrew G. Barto는 당시까지 산발적으로 연구되던 강화학습 이론들을 체계적으로 정리하여, 1998년 Reinforcement Learning: An Introduction이라는 책을 출간하였다. 이 책은 사실상 강화학습의 바이블로 자리 잡게 되었고, 이후 강화학습을 공부하는 연구자라면 누구나 반드시 참고하는 필수 자료가 되었다. 20년이 지난 2018년, 이 책의 제2판이 출간되었다. 이는 딥러닝과 강화학습이 본격적으로 결합되면서 강화학습이 새로운 시대에 진입했음을 상징적으로 보여주는 사건이었다. 2판은 수학적 깊이와 폭넓은 주제 범위로 인해 지금도 강화학습 전 분야를 아우르는 핵심 참고서로 평가받는다.

이 두 저자의 제자인 David Silver는 강화학습 분야에서 가장 주목받는 인물 중 하나이다. 그는 딥마인드에서 AlphaGo 프로젝트를 주도하였고, Q-Learning과 딥러닝을 결합하여 Atari 게임을 정복하는 데에도 중심적인 역할을 하였다.
그가 런던대(UCL)에서 대학원생을 대상으로 진행한 강화학습 강의는 지금도 슬라이드 자료와 함께 온라인에 공개되어 있으며, 많은 연구자들이 실질적인 학습 자료로 활용하고 있다.

https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-
